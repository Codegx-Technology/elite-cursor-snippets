#!/usr/bin/env python3
"""
ðŸŽ¬ REAL AI Kenya Video Generator - Using Actual AI Models
Creates authentic Kenya video using SDXL-Turbo for real image generation

// [SNIPPET]: thinkwithai + surgicalfix + refactorclean + kenyafirst
// [CONTEXT]: Fix core issues - use real AI models for actual content
// [GOAL]: Generate real Kenya images and create proper music video style
"""

import os
import sys
import time
import torch
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
from PIL import Image, ImageDraw, ImageFont

def create_real_ai_kenya_video():
    """Create Kenya video using REAL AI image generation"""
    
    print("ðŸŽ¬ REAL AI KENYA VIDEO GENERATOR")
    print("=" * 70)
    print("ðŸ¤– Using SDXL-Turbo for REAL image generation")
    print("ðŸ‡°ðŸ‡ª Creating authentic Kenya visuals")
    print("ðŸŽµ Music video style with captions")
    print("=" * 70)
    
    # Check if we can use AI models
    if not torch.cuda.is_available():
        print("âš ï¸ No CUDA available, using CPU (slower)")
    
    # Video settings
    width, height = 1920, 1080
    fps = 30
    duration_per_scene = 8  # 8 seconds per scene
    
    # Kenya script scenes with specific prompts for AI generation
    kenya_scenes = [
        {
            "text": "Kenya Yetu - Our Beautiful Homeland",
            "prompt": "Beautiful panoramic view of Kenya landscape with Mount Kenya in background, golden sunset, African savanna, photorealistic, cinematic lighting",
            "duration": duration_per_scene
        },
        {
            "text": "Mount Kenya - Snow-Capped Majesty", 
            "prompt": "Mount Kenya snow-capped peak, dramatic clouds, alpine vegetation, morning light, majestic mountain landscape, photorealistic",
            "duration": duration_per_scene
        },
        {
            "text": "Diani Beach - Tropical Paradise",
            "prompt": "Diani Beach Kenya, white sand, turquoise Indian Ocean, palm trees, tropical paradise, crystal clear water, photorealistic",
            "duration": duration_per_scene
        },
        {
            "text": "Maasai Mara - Wildlife Kingdom",
            "prompt": "Maasai Mara Kenya, lions in golden grassland, acacia trees, African wildlife, Great Migration, cinematic wildlife photography",
            "duration": duration_per_scene
        },
        {
            "text": "Nairobi - Green City Innovation",
            "prompt": "Nairobi skyline, modern buildings, green spaces, urban innovation, African city development, sunset lighting, photorealistic",
            "duration": duration_per_scene
        },
        {
            "text": "Eliud Kipchoge - Marathon Legend",
            "prompt": "Athletic runner in Kenya landscape, marathon champion, determination, African athlete, inspiring sports photography, golden hour",
            "duration": duration_per_scene
        }
    ]
    
    # Create output directory
    output_dir = Path("output")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    video_path = output_dir / f"kenya_real_ai_{timestamp}.mp4"
    
    print(f"ðŸ“ Creating: {video_path}")
    
    try:
        # Initialize AI image generator
        print("ðŸ¤– Initializing SDXL-Turbo...")
        image_generator = initialize_ai_generator()
        
        if image_generator is None:
            print("âŒ AI generator failed, creating enhanced version...")
            return create_enhanced_video_fallback(video_path, kenya_scenes, width, height, fps)
        
        # Generate real AI images for each scene
        print("ðŸŽ¨ Generating real Kenya images with AI...")
        scene_images = []
        
        for i, scene in enumerate(kenya_scenes):
            print(f"   ðŸ–¼ï¸ Generating scene {i+1}: {scene['text']}")
            
            try:
                # Generate image using SDXL-Turbo
                image = image_generator(
                    prompt=scene['prompt'],
                    num_inference_steps=2,  # Fast generation
                    guidance_scale=0.0,     # Turbo mode
                    height=height,
                    width=width
                ).images[0]
                
                # Convert PIL to OpenCV format
                cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
                scene_images.append((cv_image, scene))
                
                print(f"      âœ… Generated: {scene['text']}")
                
            except Exception as e:
                print(f"      âŒ Failed to generate {scene['text']}: {e}")
                # Create fallback image
                fallback_image = create_fallback_image(width, height, scene)
                scene_images.append((fallback_image, scene))
        
        # Create video with real AI images
        print("ðŸŽ¬ Creating music video with AI images...")
        create_music_video_with_ai_images(video_path, scene_images, fps)
        
        if video_path.exists():
            file_size = video_path.stat().st_size / (1024 * 1024)
            
            print("ðŸŽ‰ REAL AI KENYA VIDEO COMPLETE!")
            print("=" * 70)
            print(f"ðŸ“ Video: {video_path}")
            print(f"ðŸ“ Size: {file_size:.1f} MB")
            print(f"â±ï¸ Duration: {len(kenya_scenes) * duration_per_scene} seconds")
            print(f"ðŸ¤– AI Generated: {len(scene_images)} real Kenya scenes")
            print(f"ðŸŽµ Style: Music video with captions")
            print("=" * 70)
            
            return str(video_path)
        else:
            print("âŒ Video creation failed")
            return None
            
    except Exception as e:
        print(f"âŒ Error creating real AI video: {e}")
        import traceback
        traceback.print_exc()
        return None

def initialize_ai_generator():
    """Initialize SDXL-Turbo for real image generation"""
    
    try:
        from diffusers import AutoPipelineForText2Image
        
        print("   ðŸ”„ Loading SDXL-Turbo pipeline...")
        
        # Load SDXL-Turbo (we have this model cached)
        pipe = AutoPipelineForText2Image.from_pretrained(
            "stabilityai/sdxl-turbo",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            variant="fp16" if torch.cuda.is_available() else None
        )
        
        if torch.cuda.is_available():
            pipe = pipe.to("cuda")
            print("   ðŸ”¥ Using GPU acceleration")
        else:
            print("   ðŸ–¥ï¸ Using CPU (slower)")
        
        print("   âœ… SDXL-Turbo ready for image generation")
        return pipe
        
    except Exception as e:
        print(f"   âŒ Failed to initialize AI generator: {e}")
        return None

def create_music_video_with_ai_images(video_path, scene_images, fps):
    """Create music video style with AI-generated images and captions"""
    
    # Use proper H.264 codec for compatibility
    fourcc = cv2.VideoWriter_fourcc(*'H264')
    out = cv2.VideoWriter(str(video_path), fourcc, fps, (1920, 1080))
    
    for scene_idx, (ai_image, scene) in enumerate(scene_images):
        print(f"   ðŸŽ¬ Creating scene {scene_idx + 1}: {scene['text']}")
        
        frames_per_scene = fps * scene['duration']
        
        for frame_num in range(frames_per_scene):
            # Start with AI-generated image
            frame = ai_image.copy()
            
            # Add cinematic effects
            frame = add_cinematic_effects(frame, frame_num)
            
            # Add music video style captions
            frame = add_music_video_captions(frame, scene, frame_num, frames_per_scene)
            
            # Add progress indicator
            progress = (scene_idx * frames_per_scene + frame_num) / (len(scene_images) * frames_per_scene)
            frame = add_progress_bar(frame, progress)
            
            out.write(frame)
    
    out.release()
    print("   âœ… Music video created with AI images")

def add_cinematic_effects(frame, frame_num):
    """Add cinematic effects to AI-generated image"""
    
    # Add subtle zoom effect
    zoom_factor = 1.0 + 0.05 * (frame_num / 240.0)  # Slow zoom over 8 seconds
    height, width = frame.shape[:2]
    
    # Calculate crop for zoom
    crop_width = int(width / zoom_factor)
    crop_height = int(height / zoom_factor)
    
    start_x = (width - crop_width) // 2
    start_y = (height - crop_height) // 2
    
    # Crop and resize for zoom effect
    cropped = frame[start_y:start_y + crop_height, start_x:start_x + crop_width]
    frame = cv2.resize(cropped, (width, height))
    
    # Add film grain
    noise = np.random.randint(-5, 5, frame.shape, dtype=np.int16)
    frame = np.clip(frame.astype(np.int16) + noise, 0, 255).astype(np.uint8)
    
    return frame

def add_music_video_captions(frame, scene, frame_num, total_frames):
    """Add music video style captions"""
    
    height, width = frame.shape[:2]
    
    # Animation progress
    progress = frame_num / total_frames
    
    # Caption appears with animation
    if progress > 0.1:  # Start after 10% of scene
        caption_alpha = min(1.0, (progress - 0.1) * 3)
        
        # Create semi-transparent overlay for text
        overlay = frame.copy()
        
        # Add dark overlay at bottom for text readability
        cv2.rectangle(overlay, (0, height - 200), (width, height), (0, 0, 0), -1)
        
        # Blend overlay
        alpha = 0.6
        frame = cv2.addWeighted(frame, 1 - alpha, overlay, alpha, 0)
        
        # Add main caption
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 2.0
        thickness = 3
        
        # Get text size for centering
        text_size = cv2.getTextSize(scene['text'], font, font_scale, thickness)[0]
        text_x = (width - text_size[0]) // 2
        text_y = height - 100
        
        # Add text shadow
        cv2.putText(frame, scene['text'], (text_x + 3, text_y + 3), 
                   font, font_scale, (0, 0, 0), thickness + 2)
        
        # Add main text
        text_color = (255, 255, 255)  # White text
        cv2.putText(frame, scene['text'], (text_x, text_y), 
                   font, font_scale, text_color, thickness)
        
        # Add Kenya flag colors accent
        flag_colors = [(0, 0, 0), (0, 0, 255), (0, 255, 0)]  # Black, Red, Green
        for i, color in enumerate(flag_colors):
            y_pos = height - 50 + i * 15
            cv2.rectangle(frame, (50, y_pos), (width - 50, y_pos + 10), color, -1)
    
    return frame

def add_progress_bar(frame, progress):
    """Add progress bar to show video progress"""
    
    height, width = frame.shape[:2]
    
    # Progress bar at top
    bar_height = 5
    bar_width = width - 100
    bar_x = 50
    bar_y = 30
    
    # Background bar
    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (100, 100, 100), -1)
    
    # Progress bar
    progress_width = int(bar_width * progress)
    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + progress_width, bar_y + bar_height), (0, 255, 0), -1)
    
    return frame

def create_fallback_image(width, height, scene):
    """Create fallback image when AI generation fails"""
    
    # Create gradient background
    image = np.zeros((height, width, 3), dtype=np.uint8)
    
    # Kenya flag colors gradient
    colors = [(0, 0, 0), (0, 0, 255), (0, 255, 0)]  # Black, Red, Green
    
    for y in range(height):
        color_index = int((y / height) * len(colors))
        color_index = min(color_index, len(colors) - 1)
        
        # Blend between colors
        if color_index < len(colors) - 1:
            blend_factor = (y / height) * len(colors) - color_index
            color1 = np.array(colors[color_index])
            color2 = np.array(colors[color_index + 1])
            color = color1 * (1 - blend_factor) + color2 * blend_factor
        else:
            color = colors[color_index]
        
        image[y, :] = color
    
    # Add scene title
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 3.0
    thickness = 5
    
    text_size = cv2.getTextSize(scene['text'], font, font_scale, thickness)[0]
    text_x = (width - text_size[0]) // 2
    text_y = height // 2
    
    # Add text shadow
    cv2.putText(image, scene['text'], (text_x + 5, text_y + 5), 
               font, font_scale, (0, 0, 0), thickness + 2)
    
    # Add main text
    cv2.putText(image, scene['text'], (text_x, text_y), 
               font, font_scale, (255, 255, 255), thickness)
    
    return image

def create_enhanced_video_fallback(video_path, scenes, width, height, fps):
    """Create enhanced video when AI generation is not available"""
    
    print("ðŸŽ¨ Creating enhanced video with improved effects...")
    
    # Use H.264 codec for better compatibility
    fourcc = cv2.VideoWriter_fourcc(*'H264')
    out = cv2.VideoWriter(str(video_path), fourcc, fps, (width, height))
    
    for scene_idx, scene in enumerate(scenes):
        print(f"   ðŸŽ¬ Creating enhanced scene {scene_idx + 1}: {scene['text']}")
        
        frames_per_scene = fps * scene['duration']
        
        for frame_num in range(frames_per_scene):
            # Create enhanced fallback image
            frame = create_fallback_image(width, height, scene)
            
            # Add cinematic effects
            frame = add_cinematic_effects(frame, frame_num)
            
            # Add music video captions
            frame = add_music_video_captions(frame, scene, frame_num, frames_per_scene)
            
            # Add progress bar
            progress = (scene_idx * frames_per_scene + frame_num) / (len(scenes) * frames_per_scene)
            frame = add_progress_bar(frame, progress)
            
            out.write(frame)
    
    out.release()
    print("   âœ… Enhanced video created")
    return str(video_path)

def main():
    """Main function"""
    
    print("ðŸŽ¬ STARTING REAL AI KENYA VIDEO CREATION")
    print("ðŸ¤– Using actual AI models for authentic content!")
    print("=" * 70)
    
    start_time = time.time()
    
    # Create the real AI video
    result = create_real_ai_kenya_video()
    
    end_time = time.time()
    generation_time = end_time - start_time
    
    if result:
        print(f"\nðŸŽ‰ REAL AI GENERATION COMPLETE!")
        print(f"â±ï¸ Time: {generation_time:.1f} seconds")
        print(f"ðŸ“ Output: {result}")
        
        # Try to open the output directory
        try:
            output_dir = Path(result).parent
            os.startfile(str(output_dir))
            print(f"ðŸ“‚ Opened: {output_dir}")
        except:
            print(f"ðŸ“‚ Check: {Path(result).parent}")
    else:
        print("âŒ Real AI generation failed")
    
    return result

if __name__ == "__main__":
    main()
